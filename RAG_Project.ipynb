{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/XV7U7XkFRhLkp+DVzAAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avish006/Rag-Project/blob/main/RAG_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "!pip install fitz\n",
        "!pip install --upgrade --force-reinstall pymupdf\n",
        "!pip install rank_bm25\n",
        "!pip install faiss-cpu"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTSoIi_K4Og",
        "outputId": "1768e97d-b417-4293-cb02-389e5f581c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Using cached pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Using cached pymupdf-1.25.4-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "Installing collected packages: pymupdf\n",
            "  Attempting uninstall: pymupdf\n",
            "    Found existing installation: PyMuPDF 1.25.4\n",
            "    Uninstalling PyMuPDF-1.25.4:\n",
            "      Successfully uninstalled PyMuPDF-1.25.4\n",
            "Successfully installed pymupdf-1.25.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBMJrq7mKPkC"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "from rank_bm25 import BM25Okapi\n",
        "import nltk\n",
        "# Download the 'punkt_tab' data package (before using `word_tokenize`)\n",
        "nltk.download('punkt_tab') # this line has been added\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting text from a pdf document\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text(\"text\") + \"\\n\"\n",
        "    return text\n",
        "text = extract_text_from_pdf('/content/Attention is all you need.pdf')"
      ],
      "metadata": {
        "id": "Y5J9PNnOP5A3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Using Recursive Character Text Splitter for dividing text into chunks of text\n",
        "def recursive_chunking(text, chunk_size=200, overlap=100):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "    return text_splitter.split_text(text)"
      ],
      "metadata": {
        "id": "LPBXMVmaLFdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the documents into chunks of text\n",
        "chunks = recursive_chunking(text)"
      ],
      "metadata": {
        "id": "RxuShDgtQWwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Transformer for creating vector embeddings of chunks\n",
        "model = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
        "embeddings = [model.encode(chunk,normalize_embeddings=True).tolist() for chunk in chunks]"
      ],
      "metadata": {
        "id": "dSoyWb78QvMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to store embeddings\n",
        "def store_embeddings_faiss(embeddings):\n",
        "    embedding_dim = len(embeddings[0])  # Get the size of each embedding\n",
        "    index = faiss.IndexFlatL2(embedding_dim)  # Create FAISS index (L2 norm)\n",
        "\n",
        "    np_embeddings = np.array(embeddings).astype('float32')  # Convert list to NumPy array\n",
        "    index.add(np_embeddings)  # Add embeddings to FAISS\n",
        "\n",
        "    faiss.write_index(index, \"vector_store.index\")  # Save index to disk\n",
        "    print(\"Embeddings stored successfully in FAISS!\")\n",
        "\n",
        "    return index"
      ],
      "metadata": {
        "id": "ZizhtYsWS8uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an vector index for embeddings\n",
        "def load_faiss_index():\n",
        "    return faiss.read_index(\"vector_store.index\")"
      ],
      "metadata": {
        "id": "xSf4UD8_vBP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = store_embeddings_faiss(embeddings)  # Store embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WIek12nvelS",
        "outputId": "4d26a497-d498-4dd1-c7d6-d903ded4924b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings stored successfully in FAISS!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_index = load_faiss_index()  # Load stored embeddings"
      ],
      "metadata": {
        "id": "BaPuJ3DXviLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize OpenRouter client\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=\"#####\",\n",
        ")"
      ],
      "metadata": {
        "id": "5noaCkuZ648n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to retrieve relevant chunks (FAISS example)\n",
        "def hybrid_search(query, index, chunks, alpha=0.5, top_k=5):\n",
        "    \"\"\"\n",
        "    Combines vector-based retrieval and BM25 keyword search to rank document chunks.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The user query.\n",
        "        index (FAISS Index): The pre-built FAISS vector store containing document embeddings.\n",
        "        chunks (list): The list of document chunks (strings).\n",
        "        alpha (float): Weight for BM25 score in [0, 1]. (1 - alpha) is used for vector similarity.\n",
        "        top_k (int): The number of top chunks to return.\n",
        "\n",
        "    Returns:\n",
        "        list: Top-k document chunks ranked by the combined score.\n",
        "    \"\"\"\n",
        "    # Step 1: Vector-based retrieval using FAISS\n",
        "    query_embedding = model.encode(query,normalize_embeddings=True)  # Ensure this uses the same embedding model used for chunks\n",
        "    query_embedding = np.array([query_embedding]).astype('float32')\n",
        "\n",
        "    # Retrieve vector distances and indices for all chunks\n",
        "    distances, _ = index.search(query_embedding, len(chunks))\n",
        "    # Convert distances to similarity scores.\n",
        "    # For L2 distance, a common transformation is similarity = 1 / (1 + distance)\n",
        "    vector_similarities = [1 / (1 + d) for d in distances[0]]\n",
        "\n",
        "    # Step 2: Keyword-based retrieval using BM25\n",
        "    # Tokenize each document chunk (lowercase for uniformity)\n",
        "    tokenized_chunks = [word_tokenize(chunk.lower()) for chunk in chunks]\n",
        "    bm25 = BM25Okapi(tokenized_chunks)\n",
        "    tokenized_query = word_tokenize(query.lower())\n",
        "    bm25_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # Step 3: Combine scores from vector search and BM25.\n",
        "    # The combined score is a weighted sum: alpha * BM25 score + (1 - alpha) * Vector similarity.\n",
        "    combined_scores = []\n",
        "    for i in range(len(chunks)):\n",
        "        combined = alpha * bm25_scores[i] + (1 - alpha) * vector_similarities[i]\n",
        "        combined_scores.append((i, combined))\n",
        "\n",
        "    # Step 4: Sort the document chunks by the combined score in descending order\n",
        "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Step 5: Get top_k indices and retrieve corresponding document chunks\n",
        "    top_indices = [idx for idx, score in combined_scores[:top_k]]\n",
        "    retrieved_chunks = [chunks[i] for i in top_indices]\n",
        "\n",
        "    return retrieved_chunks"
      ],
      "metadata": {
        "id": "UeQHSnid8ePz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a3093a-694f-4038-dd36-c3168eb66c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_step_query_rag(query, index, chunks):\n",
        "    # Step 1: Retrieve top-k relevant chunks from the vector database\n",
        "    relevant_chunks = hybrid_search(query,index ,chunks)\n",
        "\n",
        "    # Step 2: Generate a reasoning breakdown\n",
        "    reasoning_prompt = f\"\"\"\n",
        "    Let's break down this query step by step to enhance reasoning.\n",
        "\n",
        "    Query: {query}\n",
        "\n",
        "    Step 1 - Retrieve Relevant Context: {relevant_chunks}\n",
        "\n",
        "    Step 2 - Identify Key Information: Extract facts, relationships, and key details from the context.\n",
        "\n",
        "    Step 3 - Apply Logical Deduction: Use the extracted facts to form an answer with reasoning.\n",
        "\n",
        "    Step 4 - Provide a Final Answer: Give a structured and intuitive response.\n",
        "\n",
        "    Now, let's perform this step-by-step reasoning.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 3: Query the LLM with stepwise reasoning\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek/deepseek-r1:free\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert in document-based reasoning.\"},\n",
        "            {\"role\": \"user\", \"content\": reasoning_prompt}\n",
        "        ],\n",
        "        temperature=0.8,  # Increase randomness for more detailed responses\n",
        "        top_p=0.7 , # Allow for more diverse word choices\n",
        "        presence_penalty=0.35, #Allow for newer words to be used\n",
        "        frequency_penalty = 0.2, #Penalizes repeatition of words\n",
        "        max_tokens= 1600\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "Z0Og9DSp-WyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to send query to DeepSeek R1 via OpenRouter\n",
        "def query_rag(user_query, index, chunks):\n",
        "    reasoning = multi_step_query_rag(user_query, index,chunks)\n",
        "    retrieved_chunks = hybrid_search(user_query, index, chunks)  # Get top matching chunks\n",
        "\n",
        "    # Construct full prompt with retrieved context\n",
        "    full_prompt = f\"\"\"\n",
        "      You are an advanced AI system specializing in deep reasoning and retrieval-augmented generation (RAG). Your goal is to **analyze** the retrieved context, apply **multi-step logical reasoning**, and generate a **well-structured, insightful** response.\n",
        "\n",
        "      ---\n",
        "      ## ** Step 1: Understanding the Query**\n",
        "      **User Query:**\n",
        "      {user_query}\n",
        "\n",
        "      ---\n",
        "      ## **Step 2: Retrieving Relevant Information**\n",
        "      The following context has been retrieved from the document using FAISS vector search:\n",
        "      {retrieved_chunks}\n",
        "\n",
        "      ---\n",
        "\n",
        "      ## **Step 3: Multi-Step Reasoning Beyond Retrieval**\n",
        "      To generate the best possible response, follow this structured thought process:\n",
        "\n",
        "      **Direct Extraction (if possible):**\n",
        "      - Identify **explicit** answers in the retrieved context.\n",
        "      - If the answer is **fully present**, structure it for clarity.\n",
        "\n",
        "      **Inference & Deduction (if required):**\n",
        "      - If the answer is **not explicitly stated**, use logical inference based on the given context.\n",
        "      - Identify **patterns, relationships, or missing links** to construct a complete answer.\n",
        "\n",
        "      **External Knowledge Integration (if needed):**\n",
        "      - If retrieval provides partial data, combine it with **general reasoning or background knowledge** to improve accuracy.\n",
        "      - Ensure the information is **logically consistent** and does not introduce hallucinations.\n",
        "\n",
        "      **Contextual Linking & Deep Reasoning:**\n",
        "      - Connect different retrieved chunks **logically** to form a complete, well-rounded response.\n",
        "      - Compare multiple sources, **resolve contradictions**, and extract the most reliable answer.\n",
        "\n",
        "      **Abstract Interpretation & Implications:**\n",
        "      - If applicable, go beyond factual retrieval to provide a **higher-level understanding**.\n",
        "      - Explain **why** the information matters, potential implications, or how it fits into a broader concept.\n",
        "\n",
        "      **Refer to Reasoning Text:**\n",
        "      - {reasoning}\n",
        "\n",
        "      ---\n",
        "      ## **Step 4: Generate the Final Answer**\n",
        "      Now, generate a **detailed, structured, and logically sound response** that follows the above reasoning process.\n",
        "\n",
        "      **Your response should be:**\n",
        "      **Comprehensive** → Cover key details from the retrieved context.\n",
        "      **Logical** → Show the **step-by-step** reasoning process.\n",
        "      **Insightful** → Provide interpretation and implications where necessary.\n",
        "      **Concise but Informative** → Avoid unnecessary repetition.\n",
        "\n",
        "      *If uncertainty exists, clearly state the limitations rather than making up information.*\n",
        "      \"\"\"\n",
        "\n",
        "\n",
        "    # Send request to DeepSeek R1 via OpenRouter\n",
        "    completion = client.chat.completions.create(\n",
        "        extra_headers={\n",
        "            \"HTTP-Referer\": \"<YOUR_SITE_URL>\",\n",
        "            \"X-Title\": \"<YOUR_SITE_NAME>\",\n",
        "        },\n",
        "        model=\"qwen/qwen2.5-vl-72b-instruct:free\",# deepseek/deepseek-r1:free\n",
        "        messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
        "        temperature=0.7,  # Increase randomness for more detailed responses\n",
        "        top_p=0.7 , # Allow for more diverse word choices\n",
        "        presence_penalty=0.2, #Allow for newer words to be used\n",
        "        frequency_penalty = 0.2, #Penalizes repeatition of words\n",
        "        max_tokens= 1600\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "qoChpmcxB77V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_rag(\"Explain the concept and architechture of Transformer\", index, chunks)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ari3e2BRCDEW",
        "outputId": "604fb411-a0ae-4a04-e86f-0f0d6bd42961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### **Concept and Architecture of the Transformer**\n",
            "\n",
            "#### **Concept of the Transformer**\n",
            "The Transformer is a groundbreaking neural network architecture that fundamentally shifts the paradigm in sequence-to-sequence tasks, such as machine translation, by eliminating the reliance on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). Instead, it leverages **self-attention mechanisms** to model the relationships between input and output tokens. This design choice allows the model to process all tokens in parallel, significantly enhancing computational efficiency and enabling it to capture long-range dependencies more effectively than traditional RNNs and CNNs. By relying entirely on self-attention mechanisms, the Transformer can directly model the dependencies between input and output tokens without the need for sequential processing. This innovation enables the model to handle long-range dependencies more efficiently and effectively. The core idea is to allow every token in the input sequence to interact directly with every other token, thereby capturing complex patterns and dependencies across the entire sequence. This parallel processing capability drastically reduces training time and improves performance. The self-attention mechanism enables each token to attend to all positions in the previous layer, which is a departure from the sequential nature of RNNs and the limited receptive field of CNNs. This parallelization capability is a key factor in its superior performance and scalability.\n",
            "\n",
            "#### **Architecture of the Transformer**\n",
            "The Transformer architecture is built around an **encoder-decoder structure**, which is composed of an **encoder** and a **decoder**.\n",
            "\n",
            "1. **Encoder**: \n",
            "   - The encoder processes the input sequence through a series of identical layers. Each layer consists of two sub-layers: \n",
            "     - **Multi-head Self-Attention**: This mechanism allows the model to focus on different parts of the input sequence simultaneously, capturing various aspects of the data. By splitting the attention mechanism into multiple heads, the model can learn different representations of the input, which are then concatenated and linearly transformed.\n",
            "     - **Position-wise Feed-Forward Networks**: These networks apply the same transformation to each position independently and in parallel, adding non-linearity to the model.\n",
            "   - **Positional Encoding**: Since the Transformer lacks the inherent sequential processing of RNNs, it uses positional encoding to inject information about the order of tokens in the sequence. This is crucial for tasks where the order of elements matters.\n",
            "\n",
            "2. **Decoder**: \n",
            "   - The decoder also consists of a stack of identical layers, but with an additional sub-layer: \n",
            "     - **Masked Multi-head Self-Attention**: This mechanism ensures that the decoder only attends to the previous positions in the output sequence, preventing it from \"peeking\" at future tokens during training. This is essential for autoregressive tasks like language generation.\n",
            "     - **Encoder-Decoder Attention**: This layer allows the decoder to focus on the relevant parts of the input sequence when generating each output token. It uses the output of the encoder as the key and value, and the output of the previous decoder layer as the query.\n",
            "   - Similar to the encoder, the decoder also includes position-wise feed-forward networks.\n",
            "\n",
            "#### **Impact and Applications**\n",
            "The Transformer has achieved state-of-the-art results in various natural language processing tasks, particularly in machine translation. For instance, it has outperformed previous models on English-to-German and English-to-French translation tasks, achieving higher BLEU scores at a fraction of the training cost. This efficiency and effectiveness have made the Transformer the foundation for many advanced models, including GPT and BERT.\n",
            "\n",
            "Moreover, the original Transformer model was designed by Ashish Vaswani and Illia Polosukhin, and their work has paved the way for extending the Transformer to problems involving non-text modalities, such as vision and audio. This versatility underscores the transformative impact of the Transformer architecture on the field of deep learning.\n",
            "\n",
            "#### **Conclusion**\n",
            "In summary, the Transformer's reliance on self-attention mechanisms and its parallel processing capabilities have revolutionized sequence-to-sequence tasks. Its encoder-decoder structure, combined with multi-head attention and positional encoding, enables it to capture complex dependencies and achieve superior performance. The Transformer's influence extends beyond natural language processing, making it a cornerstone of modern deep learning research and applications.\n"
          ]
        }
      ]
    }
  ]
}